---
weight: 4
title: "Mathematical tools"
---

# Matrix Calculations in Image Processing

In digital image processing, matrix calculations are essential tools that enable transformations, filtering, and various other manipulations of image data, which is often stored as matrices. These operations include element-wise operations and matrix products. Here, we’ll outline the basics of matrix operations relevant to image processing, using arithmetic and logic operations as an example.

## Primer on matrix multiplication

Matrix multiplication follows a specific order. Let’s look at the steps.
To multiply two matrices, {{< katex >}}A{{< /katex >}} and {{< katex >}}B{{< /katex >}}, and get a new matrix {{< katex >}}C{{< /katex >}}:

1. **Check dimensions**: Ensure that the number of columns in matrix {{< katex >}}A{{< /katex >}} is equal to the number of rows in matrix {{< katex >}}B{{< /katex >}}. If {{< katex >}}A{{< /katex >}} is of size {{< katex >}}m \times n{{< /katex >}} and {{< katex >}}B{{< /katex >}} is of size {{< katex >}}n \times p{{< /katex >}}, then {{< katex >}}C{{< /katex >}} will be of size {{< katex >}}m \times p{{< /katex >}}.

2. **Identify rows and columns**:
   - Each element in the resulting matrix {{< katex >}}C{{< /katex >}} is found by multiplying elements from a row in {{< katex >}}A{{< /katex >}} with elements from a column in {{< katex >}}B{{< /katex >}}.

3. **Calculate each element**:
   - To find the element in the first row and first column of {{< katex >}}C{{< /katex >}} ({{< katex >}}C_{11}{{< /katex >}}), multiply each element in the first row of {{< katex >}}A{{< /katex >}} by the corresponding element in the first column of {{< katex >}}B{{< /katex >}} and add them together.
   - Repeat this for each element in {{< katex >}}C{{< /katex >}}, moving across rows of {{< katex >}}A{{< /katex >}} and down columns of {{< katex >}}B{{< /katex >}}.

4. **General formula for each element**:
   - The element in row {{< katex >}}i{{< /katex >}} and column {{< katex >}}j{{< /katex >}} of {{< katex >}}C{{< /katex >}}, denoted as {{< katex >}}C_{ij}{{< /katex >}}, is calculated as:

     {{< katex display=true >}}
     C_{ij} = \sum_{k=1}^{n} A_{ik} \cdot B_{kj}
     {{< /katex >}}

5. **Repeat for all elements**:
   - Follow this process for each position in matrix {{< katex >}}C{{< /katex >}}, going row by row and column by column, until the entire matrix {{< katex >}}C{{< /katex >}} is filled.

In summary, the order is:

1. **Row of {{< katex >}}A{{< /katex >}}** times **Column of {{< katex >}}B{{< /katex >}}** for each element in {{< katex >}}C{{< /katex >}}.

2. **Sum the products** for each position in the result matrix.

## Element-Wise (Hadamard) Product

The element-wise product, also known as the Hadamard product, multiplies corresponding elements of two matrices. For two matrices of the same dimensions {{< katex >}}A{{< /katex >}} and {{< katex >}}B{{< /katex >}}, the element-wise product {{< katex >}}C{{< /katex >}} is calculated as follows:

{{< katex display=true >}}
C_{ij} = A_{ij} \cdot B_{ij}
{{< /katex >}}

For example, if:

{{< katex display=true >}}
A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \quad B = \begin{bmatrix} b_{11} & b_{12} \\ b_{21} & b_{22} \end{bmatrix}
{{< /katex >}}

Then the element-wise product is:

{{< katex display=true >}}
C = \begin{bmatrix} a_{11} \cdot b_{11} & a_{12} \cdot b_{12} \\ a_{21} \cdot b_{21} & a_{22} \cdot b_{22} \end{bmatrix}
{{< /katex >}}

## Matrix Multiplication (Dot Product)

Matrix multiplication (dot product) is an important operation for transformations. For this to work, the number of columns in the first matrix must match the number of rows in the second matrix. If {{< katex >}}A{{< /katex >}} is {{< katex >}}m \times n{{< /katex >}} and {{< katex >}}B{{< /katex >}} is {{< katex >}}n \times p{{< /katex >}}, then the resulting matrix {{< katex >}}C{{< /katex >}} will be {{< katex >}}m \times p{{< /katex >}}.

Each element in the new matrix, {{< katex >}}C_{ij}{{< /katex >}}, is found by taking the {{< katex >}}i{{< /katex >}}-th row of {{< katex >}}A{{< /katex >}} and the {{< katex >}}j{{< /katex >}}-th column of {{< katex >}}B{{< /katex >}}, multiplying them, and adding the results.

{{< katex display=true >}}
C_{ij} = \sum_{k=1}^{n} A_{ik} \cdot B_{kj}
{{< /katex >}}

For example:

{{< katex display=true >}}
A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \quad
{{< /katex >}}

{{< katex display=true >}}
B = \begin{bmatrix} b_{11} & b_{12} \\ b_{21} & b_{22} \end{bmatrix}
{{< /katex >}}

Then the product {{< katex >}}C = A \cdot B{{< /katex >}} is calculated as:

{{< katex display=true >}}
C = \begin{bmatrix} a_{11} \cdot b_{11} + a_{12} \cdot b_{21} & a_{11} \cdot b_{12} + a_{12} \cdot b_{22} \\ a_{21} \cdot b_{11} + a_{22} \cdot b_{21} & a_{21} \cdot b_{12} + a_{22} \cdot b_{22} \end{bmatrix}
{{< /katex >}}

### Example calculation

For matrix multiplication, let’s consider two 2x2 matrices {{< katex >}}A{{< /katex >}} and {{< katex >}}B{{< /katex >}}:

{{< katex display=true >}}
A = \begin{bmatrix} 2 & 3 \\ 1 & 4 \end{bmatrix}, \quad B = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}
{{< /katex >}}

To find the product {{< katex >}}C = A \cdot B{{< /katex >}}, we calculate each element in the resulting matrix {{< katex >}}C{{< /katex >}} using the formula:

{{< katex display=true >}}
C_{ij} = \sum_{k=1}^{n} A_{ik} \cdot B_{kj}
{{< /katex >}}

{{< katex >}}C_{11} = (2 \cdot 5) + (3 \cdot 7) = 10 + 21 = 31{{< /katex >}}.
{{< katex >}}C_{12} = (2 \cdot 6) + (3 \cdot 8) = 12 + 24 = 36{{< /katex >}}.
{{< katex >}}C_{21} = (1 \cdot 5) + (4 \cdot 7) = 5 + 28 = 33{{< /katex >}}.
{{< katex >}}C_{22} = (1 \cdot 6) + (4 \cdot 8) = 6 + 32 = 38{{< /katex >}}.

So, the resulting matrix {{< katex >}}C{{< /katex >}} is:

{{< katex display=true >}}
C = \begin{bmatrix} 31 & 36 \\ 33 & 38 \end{bmatrix}
{{< /katex >}}

## Common Matrix Operations in Image Processing

Addition and Subtraction: These are straightforward element-wise operations, useful for combining images or adjusting brightness levels.

- **Addition**: {{< katex >}}C_{ij} = A_{ij} + B_{ij}{{< /katex >}}

- **Subtraction**: {{< katex >}}C_{ij} = A_{ij} - B_{ij}{{< /katex >}}

- **Scaling**: Multiplying each element by a constant, often used for brightness or contrast adjustments. If {{< katex >}}\alpha{{< /katex >}} is a scalar, then {{< katex >}}C_{ij} = \alpha \cdot A_{ij}{{< /katex >}}.

- **Logical Operations**: Element-wise logical operations, such as AND, OR, and XOR, are often used in binary image processing.
  - **AND**: {{< katex >}}C_{ij} = A_{ij} \land B_{ij}{{< /katex >}}
  - **OR**: {{< katex >}}C_{ij} = A_{ij} \lor B_{ij}{{< /katex >}}
  - **XOR**: {{< katex >}}C_{ij} = A_{ij} \oplus B_{ij}{{< /katex >}}

These matrix operations form the basis for more complex image transformations and manipulations, such as convolution for filtering, transformations for image alignment, and various pixel-wise operations crucial to image enhancement and analysis.

## Convolution and Correlation

Convolution and correlation are fundamental operations in image processing, especially useful in neighborhood-oriented algorithms. These operations help in tasks like edge detection, blurring, sharpening, and feature extraction by transforming pixel values based on their local neighborhood.

### Convolution

Convolution combines an image with a smaller matrix, called a kernel or mask, to adjust each pixel based on its surrounding pixels. The kernel’s values determine what features, like edges or smooth areas, will stand out in the image. Convolution of a 2D function {{< katex >}}f(x, y){{< /katex >}} with a kernel {{< katex >}}h(x, y){{< /katex >}} is given by:

{{< katex display=true >}}
g(x, y) = \sum_{k=-n}^{n} \sum_{j=-m}^{m} h(j, k) \cdot f(x - j, y - k)
{{< /katex >}}

where:

- {{< katex >}}g(x, y){{< /katex >}} is the output pixel,
- {{< katex >}}f(x, y){{< /katex >}} is the input image,
- {{< katex >}}h(j, k){{< /katex >}} is the kernel (e.g., a 3x3 or 5x5 matrix),
- {{< katex >}}m{{< /katex >}} and {{< katex >}}n{{< /katex >}} are half the width and height of the kernel.

### Example of Convolution Kernels

Depending on the kernel values, different effects can be achieved:

- **Low-pass (smoothing)**: Blurs the image by averaging neighboring pixels.
- **High-pass (sharpening)**: Enhances edges and fine details.
- **Edge detection**: Highlights areas of sharp contrast by emphasizing gradients.

For instance, a basic 3x3 high-pass kernel might look like:

{{< katex display=true >}}
h = \begin{bmatrix} -1 & -1 & -1 \\ -1 & 8 & -1 \\ -1 & -1 & -1 \end{bmatrix}
{{< /katex >}}

### Correlation

Correlation is similar to convolution but without mirroring the kernel. The mathematical formula is:

{{< katex display=true >}}
g(x, y) = \sum_{k=-n}^{n} \sum_{j=-m}^{m} h(j, k) \cdot f(x + j, y + k)
{{< /katex >}}

The difference is in how the kernel interacts with the image:

- In **correlation**, the kernel’s layout is kept as is.
- In **convolution**, the kernel is mirrored both horizontally and vertically before calculation.

### When to Use Correlation vs. Convolution

In many cases, the difference between convolution and correlation is very small, especially if the kernel is symmetrical. However, convolution is often chosen in image processing because it works better with common filtering and feature extraction methods.

### Separable Kernels

A kernel is separable if it can be split into two smaller 1D kernels. Instead of applying a full 2D kernel, you can use one 1D kernel along rows and the other along columns, making the process faster.

For a kernel {{< katex >}}K{{< /katex >}} to be separable, it must satisfy the condition:

{{< katex display=true >}}
K(x, y) = k_x(x) \cdot k_y(y)
{{< /katex >}}

where {{< katex >}}k_x(x){{< /katex >}} and {{< katex >}}k_y(y){{< /katex >}} are the one-dimensional kernels.

#### Why Use Separable Kernels?

1. **Efficiency**: Applying two 1D filters is much faster than applying a full 2D filter, especially for large images. The computational cost reduces from {{< katex >}}O(n^2){{< /katex >}} to {{< katex >}}O(2n){{< /katex >}}, where {{< katex >}}n{{< /katex >}} is the kernel size.
2. **Memory**: Separable kernels require less memory as they can be stored as two smaller 1D arrays instead of a larger 2D matrix.

#### Example of a Separable Kernel

A common example of a separable kernel is the **Gaussian kernel** used in smoothing filters. A 2D Gaussian kernel can be split into two 1D Gaussian kernels, allowing it to be applied in two steps—first horizontally, then vertically.

## Commutative, Associative, Distributive Properties

In matrix arithmetic for image processing, the following properties are important:

**Commutative Property**: Only holds for element-wise operations, such as addition and multiplication, not for matrix multiplication.

{{< katex >}}A + B = B + A{{< /katex >}}

**Associative Property**: Holds for both element-wise and matrix operations. For example:

{{< katex >}}(A + B) + C = A + (B + C){{< /katex >}}

**Distributive Property**: Multiplication distributes over addition.

{{< katex >}}A \cdot (B + C) = A \cdot B + A \cdot C{{< /katex >}}

## Arithmetic Operations

**Addition**: Increases brightness when a constant is added. In matrix form, each element {{< katex >}}C_{ij}{{< /katex >}} is the sum of corresponding elements in {{< katex >}}A{{< /katex >}} and {{< katex >}}B{{< /katex >}}.

**Subtraction**: Useful for detecting changes or differences between images, such as in background subtraction.

### Handling Overflow

In image processing, when values exceed the maximum allowed range (e.g., 0-255 for 8-bit images), they may be "clipped" to the max/min values or "wrapped" around. Proper handling prevents distortion in the resulting image.

## Applications

- **Image Blending**: Adding or averaging multiple images to create smooth transitions or overlays.
- **Edge Detection**: Subtracting smoothed images to enhance edges.
- **Thresholding**: Using logic operations to isolate areas of interest in binary images.
- **Flat Field Correction**: Using arithmetic to correct lighting variations across an image.
